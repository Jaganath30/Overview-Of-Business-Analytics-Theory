{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "85cebe0e-360d-4b3b-b181-d82352f74fe6",
      "metadata": {
        "id": "85cebe0e-360d-4b3b-b181-d82352f74fe6"
      },
      "source": [
        "# Introduction to Business Analytics:\n",
        "\n",
        "\n",
        "## Definition:\n",
        "\n",
        "#### Business Analytics (BA) is the practice of <u> **iterative, methodical exploration of an organization's data** </u> with an emphasis on\n",
        "\n",
        "#### statistical analysis. It is <u> **used by companies to drive decision-making through insights derived from data** </u>.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06dad16f-6e0a-44c8-96eb-efa7db0c42f6",
      "metadata": {
        "id": "06dad16f-6e0a-44c8-96eb-efa7db0c42f6"
      },
      "source": [
        "# Types of Business Analytics\n",
        "## 1. Descriptive Analytics\n",
        "## 2. Predictive Analytics\n",
        "## 3. Prescriptive Analytics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85d4b54f-39fa-4d12-9310-b74e47b28729",
      "metadata": {
        "id": "85d4b54f-39fa-4d12-9310-b74e47b28729"
      },
      "source": [
        "# Descriptive Analytics\n",
        "### Definition: Descriptive analytics examines historical data to identify trends and patterns. It answers the question,\n",
        "###  ** <u> \"What happened?\" </u> **\n",
        "#### Key Techniques:\n",
        "* Data aggregation\n",
        "* Data mining\n",
        "* Data visualization\n",
        "* Reporting\n",
        "- Example:\n",
        "  - A retail company analyses past sales data to determine the most popular products and peak sales periods.\n",
        "  - Visualization: Creating dashboards and reports showing key performance indicators (KPIs) like total sales, average purchase value, and customer demographics.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3986a80d-72ce-4cfe-bfc5-5c04e23f6bad",
      "metadata": {
        "id": "3986a80d-72ce-4cfe-bfc5-5c04e23f6bad"
      },
      "source": [
        "### Three examples of descriptive business analysis using datasets and Python code:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df79c9a0-4c68-4c34-b87c-f4710c68eddb",
      "metadata": {
        "id": "df79c9a0-4c68-4c34-b87c-f4710c68eddb"
      },
      "source": [
        "## 1.Retail Sales Analysis:\n",
        "   - Dataset: A dataset containing information about retail sales transactions, including sales volume, revenue, product categories, and customer demographics.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de268192-f551-4e36-bb8e-548ed8ad7fa8",
      "metadata": {
        "id": "de268192-f551-4e36-bb8e-548ed8ad7fa8"
      },
      "source": [
        "- Python Code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30a4b521-c8d3-4a14-a22b-0c34f2117049",
      "metadata": {
        "id": "30a4b521-c8d3-4a14-a22b-0c34f2117049",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "    #python\n",
        "import pandas as pd\n",
        "\n",
        "     # Load dataset\n",
        "retail_data = pd.read_csv('retail_sales_data.csv')\n",
        "\n",
        "     # Basic statistics\n",
        "print(retail_data.describe())\n",
        "\n",
        "     # Total revenue\n",
        "total_revenue = retail_data['Revenue'].sum()\n",
        "print(\"Total Revenue:\", total_revenue)\n",
        "\n",
        "     # Sales by product category\n",
        "sales_by_category = retail_data.groupby('Category')['Sales'].sum()\n",
        "print(\"Sales by Category:\")\n",
        "print(sales_by_category)\n",
        "\n",
        "     # Customer demographics analysis\n",
        "age_distribution = retail_data['Age'].value_counts()\n",
        "print(\"Age Distribution of Customers:\")\n",
        "print(age_distribution)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3be35962-3ddf-4c1e-a6b0-94eda9da893c",
      "metadata": {
        "id": "3be35962-3ddf-4c1e-a6b0-94eda9da893c"
      },
      "source": [
        "# 2. **Website Traffic Analysis:**\n",
        "\n",
        "###    - Dataset: Web server logs containing information about website visits, including timestamps, IP addresses, page views, and referral sources.\n",
        "\n",
        "####   - Python Code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed20ce19-72f1-4559-807c-9a5d8abf9ef8",
      "metadata": {
        "id": "ed20ce19-72f1-4559-807c-9a5d8abf9ef8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "     # Load dataset\n",
        "web_logs = pd.read_csv('web_server_logs.csv')\n",
        "\n",
        "     # Total number of visits\n",
        "total_visits = len(web_logs)\n",
        "print(\"Total Visits:\", total_visits)\n",
        "\n",
        "     # Page views per day\n",
        "web_logs['Date'] = pd.to_datetime(web_logs['Timestamp']).dt.date\n",
        "daily_page_views = web_logs.groupby('Date')['PageViews'].sum()\n",
        "print(\"Daily Page Views:\")\n",
        "print(daily_page_views)\n",
        "\n",
        "     # Referral sources analysis\n",
        "referral_analysis = web_logs['ReferralSource'].value_counts()\n",
        "print(\"Referral Sources Analysis:\")\n",
        "print(referral_analysis)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2451a8b9-6b2e-4797-9768-baecc63edb94",
      "metadata": {
        "id": "2451a8b9-6b2e-4797-9768-baecc63edb94"
      },
      "source": [
        "# 3. **Employee Performance Analysis:**\n",
        "###   - Dataset: HR data containing information about employee performance, including ratings, salary, tenure, and department.\n",
        "####   - Python Code:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b0e2a6d-b043-4474-acc0-900d9ff3aad7",
      "metadata": {
        "id": "7b0e2a6d-b043-4474-acc0-900d9ff3aad7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "     # Load dataset\n",
        "hr_data = pd.read_csv('employee_performance_data.csv')\n",
        "\n",
        "     # Average performance rating\n",
        "avg_rating = hr_data['PerformanceRating'].mean()\n",
        "     print(\"Average Performance Rating:\", avg_rating)\n",
        "\n",
        "     # Salary distribution by department\n",
        "     salary_by_department = hr_data.groupby('Department')['Salary'].mean()\n",
        "     print(\"Salary Distribution by Department:\")\n",
        "     print(salary_by_department)\n",
        "\n",
        "     # Tenure analysis\n",
        "     tenure_analysis = hr_data['Tenure'].describe()\n",
        "     print(\"Tenure Analysis:\")\n",
        "     print(tenure_analysis)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f07b0a8e-f977-49d2-9620-34b5bc0b2077",
      "metadata": {
        "id": "f07b0a8e-f977-49d2-9620-34b5bc0b2077"
      },
      "source": [
        "\n",
        "### These examples demonstrate basic descriptive analysis techniques using Python and common datasets from different business domains. You can further expand upon these analyses by incorporating visualization libraries like Matplotlib or Seaborn for better insights and presentations\n",
        "\n",
        "### Predictive Analytics\n",
        "- *Definition*: Predictive analytics uses historical data to make predictions about future events. It answers the question, \"What could happen?\"\n",
        "- *Key Techniques*:\n",
        "  - Machine learning\n",
        "  - Statistical modeling\n",
        "  - Forecasting\n",
        "- *Example*:\n",
        "  - An e-commerce company uses predictive analytics to forecast future sales based on historical sales data, customer behavior, and market trends.\n",
        "  - *Technique*: Using regression analysis to predict future sales volumes or employing machine learning algorithms to predict customer churn rates.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3 Examples of PREDICTIVE BUSINESS ANALYTICS:\n",
        "Certainly! Here are three examples of predictive business analytics problems along with corresponding datasets and Python code:\n",
        "\n",
        "### Example 1: Predicting Customer Churn\n",
        "\n",
        "**Problem Statement:** Predict whether a customer will churn (leave the service) based on their usage data.\n",
        "\n",
        "**Dataset:** Telecom Churn Dataset\n",
        "\n",
        "\n",
        "**Python Code:**"
      ],
      "metadata": {
        "id": "Y4s8u-hwwG1k"
      },
      "id": "Y4s8u-hwwG1k"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/stedy/Machine-Learning-with-R-datasets/master/WA_Fn-UseC_-Telco-Customer-Churn.csv')\n",
        "\n",
        "# Data Preprocessing\n",
        "df = df.drop(columns=['customerID'])  # Drop irrelevant column\n",
        "df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')  # Convert to numeric\n",
        "df = df.dropna()  # Drop missing values\n",
        "df['Churn'] = df['Churn'].apply(lambda x: 1 if x == 'Yes' else 0)  # Convert target variable to binary\n",
        "\n",
        "# Feature encoding\n",
        "df = pd.get_dummies(df, drop_first=True)\n",
        "\n",
        "# Train-test split\n",
        "X = df.drop(columns=['Churn'])\n",
        "y = df['Churn']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Model training\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "crGRr3uFwRbh"
      },
      "id": "crGRr3uFwRbh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 2: Sales Forecasting\n",
        "\n",
        "**Problem Statement:** Predict future sales for a retail store based on historical sales data.\n",
        "\n",
        "**Dataset:** Rossmann Store Sales Dataset\n",
        "\n",
        "**Python Code:**\n"
      ],
      "metadata": {
        "id": "UmIBfLj_xRGX"
      },
      "id": "UmIBfLj_xRGX"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Load dataset\n",
        "train = pd.read_csv('https://raw.githubusercontent.com/ogozuacik/rossmann-store-sales/master/train.csv')\n",
        "store = pd.read_csv('https://raw.githubusercontent.com/ogozuacik/rossmann-store-sales/master/store.csv')\n",
        "\n",
        "# Merge datasets\n",
        "df = pd.merge(train, store, on='Store')\n",
        "\n",
        "# Data Preprocessing\n",
        "df = df[df['Open'] == 1]\n",
        "df = df.drop(columns=['Open', 'PromoInterval', 'CompetitionOpenSinceMonth', 'CompetitionOpenSinceYear'])\n",
        "\n",
        "# Feature engineering\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "df['Year'] = df['Date'].dt.year\n",
        "df['Month'] = df['Date'].dt.month\n",
        "df['Day'] = df['Date'].dt.day\n",
        "df = df.drop(columns=['Date'])\n",
        "\n",
        "# Feature encoding\n",
        "df = pd.get_dummies(df, drop_first=True)\n",
        "\n",
        "# Train-test split\n",
        "X = df.drop(columns=['Sales'])\n",
        "y = df['Sales']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Model training\n",
        "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "print(\"Mean Absolute Error:\", mean_absolute_error(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "cVY-IzdoxbZ3"
      },
      "id": "cVY-IzdoxbZ3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 3: Credit Risk Assessment\n",
        "\n",
        "**Problem Statement:** Predict the likelihood of a loan applicant defaulting on a loan based on their financial and personal information.\n",
        "\n",
        "**Dataset:** LendingClub Loan Data\n",
        "\n",
        "**Python Code:**\n"
      ],
      "metadata": {
        "id": "Q8K4o1cvy82f"
      },
      "id": "Q8K4o1cvy82f"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/ben519/DataWrangling/master/Data/lending-club-loan-data/loan.csv')\n",
        "\n",
        "# Data Preprocessing\n",
        "df = df[['loan_amnt', 'term', 'int_rate', 'installment', 'grade', 'emp_length', 'home_ownership', 'annual_inc', 'loan_status']]\n",
        "df = df.dropna()  # Drop missing values\n",
        "df = df[df['loan_status'].isin(['Fully Paid', 'Charged Off'])]  # Binary classification\n",
        "df['loan_status'] = df['loan_status'].apply(lambda x: 1 if x == 'Charged Off' else 0)  # Convert target to binary\n",
        "\n",
        "# Feature encoding\n",
        "le = LabelEncoder()\n",
        "df['term'] = le.fit_transform(df['term'])\n",
        "df['grade'] = le.fit_transform(df['grade'])\n",
        "df['emp_length'] = le.fit_transform(df['emp_length'])\n",
        "df['home_ownership'] = le.fit_transform(df['home_ownership'])\n",
        "\n",
        "# Train-test split\n",
        "X = df.drop(columns=['loan_status'])\n",
        "y = df['loan_status']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Model training\n",
        "model = GradientBoostingClassifier(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "poSHzdbbzIsZ"
      },
      "id": "poSHzdbbzIsZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These examples illustrate the use of Python and its libraries to solve common predictive business analytics problems. Each example includes data preprocessing, feature engineering, model training, and evaluation to provide a complete workflow.\n",
        "\n",
        "### Prescriptive Analytics\n",
        "- *Definition*: Prescriptive analytics provides recommendations for decision-making by analyzing data to determine the best course of action. It answers the question, \"What should we do?\"\n",
        "- *Key Techniques*:\n",
        "  - Optimization\n",
        "  - Simulation\n",
        "  - Decision analysis\n"
      ],
      "metadata": {
        "id": "pOgUED5DzUfQ"
      },
      "id": "pOgUED5DzUfQ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Example*:\n",
        "  - A logistics company uses prescriptive analytics to optimize delivery routes, reducing fuel costs and improving delivery times.\n",
        "  - *Technique*: Employing optimization algorithms to find the most efficient route based on traffic data, delivery locations, and vehicle constraints.\n"
      ],
      "metadata": {
        "id": "I9mwzGxPzg-f"
      },
      "id": "I9mwzGxPzg-f"
    },
    {
      "cell_type": "markdown",
      "source": [
        "3 Examples of PRESCRIPTIVE BUSINESS ANALYTICS:\n",
        "Certainly! Prescriptive analysis involves recommending actions or decisions based on analysis of data. Here are three examples:\n",
        "\n",
        "1. **Customer Churn Prediction and Retention Strategy:**\n",
        "   - Dataset: Contains customer information such as demographics, usage patterns, and churn status (whether they left or stayed).\n",
        "   - Python Code:\n"
      ],
      "metadata": {
        "id": "CDnE96x-ztMR"
      },
      "id": "CDnE96x-ztMR"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "customer_data = pd.read_csv('customer_churn_data.csv')\n",
        "\n",
        "# Data preprocessing\n",
        "\n",
        "# Split data into features and target\n",
        "X = customer_data.drop('Churn', axis=1)\n",
        "y = customer_data['Churn']\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Model training\n",
        "model = RandomForestClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Model evaluation\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Identify key features influencing churn\n",
        "feature_importance = pd.Series(model.feature_importances_, index=X.columns)\n",
        "print(\"Feature Importance:\")\n",
        "print(feature_importance)\n",
        "\n",
        "# Recommend retention strategies based on analysis"
      ],
      "metadata": {
        "id": "TtSEKPgszuvv"
      },
      "id": "TtSEKPgszuvv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **Inventory Optimization:**\n",
        "   - Dataset: Contains historical sales data, lead times, and inventory levels for different products.\n",
        "   - Python Code:\n"
      ],
      "metadata": {
        "id": "82RdmU981iAX"
      },
      "id": "82RdmU981iAX"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "# Load dataset\n",
        "sales_data = pd.read_csv('sales_data.csv')\n",
        "\n",
        "# Define optimization function\n",
        "def inventory_cost(x):\n",
        "    # x: inventory levels for each product\n",
        "    # Calculate total holding and stockout costs\n",
        "    holding_cost = (x * holding_cost_per_unit).sum()\n",
        "    stockout_cost = max(0, (demand_forecast - x).sum() * stockout_cost_per_unit)\n",
        "    total_cost = holding_cost + stockout_cost\n",
        "    return total_cost\n",
        "\n",
        "# Define constraints\n",
        "constraints = ({'type': 'ineq', 'fun': lambda x: demand_forecast - x})\n",
        "\n",
        "# Initial guess for inventory levels\n",
        "initial_guess = [100, 150, 200]  # Example initial inventory levels\n",
        "\n",
        "# Perform optimization\n",
        "result = minimize(inventory_cost, initial_guess, constraints=constraints)\n",
        "\n",
        "# Recommend optimal inventory levels\n",
        "optimal_inventory_levels = result.x\n",
        "print(\"Optimal Inventory Levels:\", optimal_inventory_levels)\n"
      ],
      "metadata": {
        "id": "gDlLtxdM1pup"
      },
      "id": "gDlLtxdM1pup",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. **Marketing Campaign Optimization:**\n",
        "   - Dataset: Contains customer demographic data, past campaign response rates, and channel preferences.\n",
        "   - Python Code:\n"
      ],
      "metadata": {
        "id": "ENWmHdO41weH"
      },
      "id": "ENWmHdO41weH"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "campaign_data = pd.read_csv('marketing_campaign_data.csv')\n",
        "\n",
        "# Data preprocessing\n",
        "\n",
        "# Split data into features and target\n",
        "X = campaign_data.drop('Response', axis=1)\n",
        "y = campaign_data['Response']\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Model training\n",
        "model = RandomForestClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Model evaluation\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Use model to predict response for new leads and allocate marketing budget accordingly\n"
      ],
      "metadata": {
        "id": "LZ9lwrCc11TP"
      },
      "id": "LZ9lwrCc11TP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These examples demonstrate how prescriptive analysis can be used to make informed decisions and recommendations based on data analysis in various business scenarios.\n",
        "\n",
        "### Integration and Importance\n",
        "- *Data Integration*: Combining data from different sources for comprehensive analysis.\n",
        "- *Business Intelligence (BI)*: Utilizing BI tools to transform raw data into meaningful insights for strategic decision-making.\n",
        "- *Competitive Advantage*: Businesses leveraging analytics can better understand their market, improve efficiency, enhance customer satisfaction, and gain a competitive edge.\n"
      ],
      "metadata": {
        "id": "asM6NS402AiR"
      },
      "id": "asM6NS402AiR"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Case Studies\n",
        "1. Descriptive Analytics:\n",
        "   - *Walmart* uses descriptive analytics to analyze customer purchasing patterns to optimize inventory and ensure popular products are always in stock.\n",
        "PYTHON CODE :\n",
        "### Descriptive Analytics Case Study: Walmart Customer Purchasing Patterns\n",
        "\n",
        "**Objective:** Analyze customer purchasing patterns to optimize inventory and ensure popular products are always in stock.\n",
        "\n",
        "**Dataset:** Walmart Sales Data (example datasets can be sourced from Kaggle or other public repositories)\n",
        "\n",
        "**Python Code:**\n",
        "\n",
        "Below is an example code using a hypothetical Walmart sales dataset. We will perform descriptive analytics to analyze customer purchasing patterns.\n",
        "\n",
        "**Step 1: Load the Data**\n",
        "\n",
        "Download the dataset from a source such as Kaggle. For this example, we assume the dataset is named `walmart_sales.csv`.\n",
        "\n",
        "```python\n"
      ],
      "metadata": {
        "id": "OxlhEOBI7H3g"
      },
      "id": "OxlhEOBI7H3g"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv('walmart_sales.csv')\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "df.head()"
      ],
      "metadata": {
        "id": "haC5V9ky7V5Y"
      },
      "id": "haC5V9ky7V5Y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Step 2: Data Preprocessing**\n",
        "\n",
        "Ensure data quality by handling missing values and converting data types if necessary.\n"
      ],
      "metadata": {
        "id": "WqBqgvPe7b9k"
      },
      "id": "WqBqgvPe7b9k"
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Drop rows with missing values (or handle them accordingly)\n",
        "df = df.dropna()\n",
        "\n",
        "# Convert data types if necessary (e.g., Date column to datetime)\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n"
      ],
      "metadata": {
        "id": "IFtCxu2Q7fur"
      },
      "id": "IFtCxu2Q7fur",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Step 3: Exploratory Data Analysis (EDA)**\n",
        "\n",
        "Perform basic descriptive analytics to understand purchasing patterns.\n",
        "\n"
      ],
      "metadata": {
        "id": "PZjGFHEF7mbl"
      },
      "id": "PZjGFHEF7mbl"
    },
    {
      "cell_type": "code",
      "source": [
        "# Summary statistics\n",
        "print(df.describe())\n",
        "\n",
        "# Sales over time\n",
        "sales_over_time = df.groupby('Date')['Weekly_Sales'].sum().reset_index()\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(sales_over_time['Date'], sales_over_time['Weekly_Sales'])\n",
        "plt.title('Total Weekly Sales Over Time')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Weekly Sales')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xGjUq7gu7qcy"
      },
      "id": "xGjUq7gu7qcy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4: Analyzing Top Products**\n",
        "\n",
        "Identify the most popular products based on sales.\n"
      ],
      "metadata": {
        "id": "I4PsIkzB7ycp"
      },
      "id": "I4PsIkzB7ycp"
    },
    {
      "cell_type": "code",
      "source": [
        "# Total sales per product\n",
        "product_sales = df.groupby('Product')['Weekly_Sales'].sum().reset_index()\n",
        "\n",
        "# Sort products by sales\n",
        "top_products = product_sales.sort_values(by='Weekly_Sales', ascending=False).head(10)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.bar(top_products['Product'], top_products['Weekly_Sales'])\n",
        "plt.title('Top 10 Products by Total Sales')\n",
        "plt.xlabel('Product')\n",
        "plt.ylabel('Total Sales')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Ix8u75V373IJ"
      },
      "id": "Ix8u75V373IJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Step 5: Seasonal Trends**\n",
        "\n",
        "Analyze sales trends across different seasons.\n"
      ],
      "metadata": {
        "id": "kCDdX1LL78z3"
      },
      "id": "kCDdX1LL78z3"
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract month and year from the Date column\n",
        "df['Month'] =\n",
        "\n",
        "df['Date'].dt.month\n",
        "df['Year'] = df['Date'].dt.year\n",
        "\n",
        "# Aggregate sales by month\n",
        "monthly_sales = df.groupby(['Year', 'Month'])['Weekly_Sales'].sum().reset_index()\n",
        "\n",
        "# Pivot the table to have years as columns and months as rows\n",
        "monthly_sales_pivot = monthly_sales.pivot(index='Month', columns='Year', values='Weekly_Sales')\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "monthly_sales_pivot.plot(kind='bar', figsize=(12, 6))\n",
        "plt.title('Monthly Sales Trends Over the Years')\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Total Sales')\n",
        "plt.legend(title='Year')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Kc0sPhva8A8n"
      },
      "id": "Kc0sPhva8A8n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 6: Store Performance**\n",
        "\n",
        "Analyze sales performance across different stores.\n",
        "\n"
      ],
      "metadata": {
        "id": "_cFDdxZ-8J_9"
      },
      "id": "_cFDdxZ-8J_9"
    },
    {
      "cell_type": "code",
      "source": [
        "# Total sales per store\n",
        "store_sales = df.groupby('Store')['Weekly_Sales'].sum().reset_index()\n",
        "\n",
        "# Sort stores by sales\n",
        "top_stores = store_sales.sort_values(by='Weekly_Sales', ascending=False).head(10)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.bar(top_stores['Store'], top_stores['Weekly_Sales'])\n",
        "plt.title('Top 10 Stores by Total Sales')\n",
        "plt.xlabel('Store')\n",
        "plt.ylabel('Total Sales')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "RdPhTD5k8N4z"
      },
      "id": "RdPhTD5k8N4z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 7: Insights and Recommendations**\n",
        "\n",
        "Using the insights obtained from the descriptive analysis, provide actionable recommendations:\n",
        "\n",
        "1. **Inventory Optimization:**\n",
        "   - Ensure that the top-selling products are always in stock, especially during peak seasons identified from the seasonal trends analysis.\n",
        "   - Monitor inventory levels more closely in the top-performing stores to avoid stockouts.\n",
        "\n",
        "2. **Marketing and Promotions:**\n",
        "   - Implement targeted marketing campaigns for products with high sales potential but currently low sales volume.\n",
        "   - Use historical sales data to plan promotional activities around peak sales periods.\n",
        "\n",
        "3. **Store Operations:**\n",
        "   - Allocate more resources to the top-performing stores to enhance customer service and increase sales further.\n",
        "   - Analyze store-specific factors contributing to lower sales in underperforming stores and implement improvement strategies.\n"
      ],
      "metadata": {
        "id": "hWaPq72a8UAV"
      },
      "id": "hWaPq72a8UAV"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example Dataset\n",
        "\n",
        "Here is a sample structure for the `walmart_sales.csv` dataset:\n",
        "\n",
        "```csv\n",
        "Date,Store,Product,Weekly_Sales\n",
        "2012-02-03,1,Product1,24924.50\n",
        "2012-02-03,1,Product2,16145.35\n",
        "2012-02-03,2,Product1,25635.20\n",
        "2012-02-03,2,Product3,21435.15\n",
        "2012-02-10,1,Product1,23745.90\n",
        "2012-02-10,1,Product2,15367.85\n"
      ],
      "metadata": {
        "id": "HqKgiywh8bWB"
      },
      "id": "HqKgiywh8bWB"
    },
    {
      "cell_type": "markdown",
      "source": [
        "By following the steps above with the actual dataset, you can perform descriptive analytics to gain insights into customer purchasing patterns, helping Walmart optimize inventory and improve overall sales performance.\n",
        "2. *Predictive Analytics*:\n",
        "   - *Netflix* uses predictive analytics to recommend shows and movies to users based on their viewing history, enhancing user experience and engagement.\n",
        "PYTHON CODE:\n",
        "While I cannot provide you with a dataset from Netflix, I can give you a simplified example using a hypothetical dataset for movie recommendations. We can use collaborative filtering, a common technique for recommendation systems.\n",
        "\n",
        "Here's how you can implement it in Python:\n"
      ],
      "metadata": {
        "id": "jmH0i4V78lqW"
      },
      "id": "jmH0i4V78lqW"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt\n",
        "\n",
        "# Hypothetical movie ratings dataset\n",
        "ratings_data = {\n",
        "    'User': ['User1', 'User1', 'User1', 'User2', 'User2', 'User3'],\n",
        "    'Movie': ['Movie1', 'Movie2', 'Movie3', 'Movie1', 'Movie2', 'Movie3'],\n",
        "    'Rating': [4, 5, 3, 5, 4, 2]\n",
        "}\n",
        "\n",
        "ratings_df = pd.DataFrame(ratings_data)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "train_data, test_data = train_test_split(ratings_df, test_size=0.2)\n",
        "\n",
        "# Create user-item matrix\n",
        "user_item_matrix = train_data.pivot_table(index='User', columns='Movie', values='Rating')\n",
        "\n",
        "# Function to calculate similarity between users\n",
        "def similarity(user1, user2):\n",
        "    rated_movies_user1 = user_item_matrix.loc[user1].dropna().index\n",
        "    rated_movies_user2 = user_item_matrix.loc[user2].dropna().index\n",
        "    common_movies = list(set(rated_movies_user1) & set(rated_movies_user2))\n",
        "\n",
        "    if len(common_movies) == 0:\n",
        "        return 0\n",
        "\n",
        "    user1_ratings = user_item_matrix.loc[user1, common_movies]\n",
        "    user2_ratings = user_item_matrix.loc[user2, common_movies]\n",
        "\n",
        "    return user1_ratings.corr(user2_ratings)\n",
        "\n",
        "# Function to predict ratings\n",
        "def predict_rating(user, movie):\n",
        "    similar_users = user_item_matrix.index.to_list()\n",
        "    similar_users.remove(user)\n",
        "\n",
        "    numerator = 0\n",
        "    denominator = 0\n",
        "\n",
        "    for other_user in similar_users:\n",
        "        sim = similarity(user, other_user)\n",
        "        if sim > 0:\n",
        "            rating = user_item_matrix.loc[other_user, movie]\n",
        "            if not pd.isnull(rating):\n",
        "                numerator += sim * rating\n",
        "                denominator += sim\n",
        "\n",
        "    if denominator == 0:\n",
        "        return 0\n",
        "    else:\n",
        "        return numerator / denominator\n",
        "\n",
        "# Function to evaluate model\n",
        "def evaluate_model(test_data):\n",
        "    predictions = []\n",
        "    for index, row in test_data.iterrows():\n",
        "        user = row['User']\n",
        "        movie = row['Movie']\n",
        "        predicted_rating = predict_rating(user, movie)\n",
        "        predictions.append(predicted_rating)\n",
        "\n",
        "    test_data['PredictedRating'] = predictions\n",
        "    rmse = sqrt(mean_squared_error(test_data['Rating'], test_data['PredictedRating']))\n",
        "    return rmse\n",
        "\n",
        "# Evaluate model\n",
        "rmse = evaluate_model(test_data)\n",
        "print(\"Root Mean Squared Error:\", rmse)"
      ],
      "metadata": {
        "id": "bWHMNXOZ8m6M"
      },
      "id": "bWHMNXOZ8m6M",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code sets up a simple collaborative filtering recommendation system based on movie ratings. The `ratings_data` represents hypothetical movie ratings provided by users. The code then splits the data into training and testing sets, calculates user similarities, predicts ratings for movies, and evaluates the model's performance using Root Mean Squared Error (RMSE).\n",
        "\n",
        "For a real-world scenario like Netflix, you'd use much larger datasets and more sophisticated algorithms, but this example illustrates the basic concepts.\n"
      ],
      "metadata": {
        "id": "-32J0Dc98w7h"
      },
      "id": "-32J0Dc98w7h"
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. *Prescriptive Analytics*:\n",
        "   - *Uber* uses prescriptive analytics to determine dynamic pricing based on real-time demand and supply, optimizing driver allocation and maximizing revenue.\n",
        "Creating a complete simulation of Uber's dynamic pricing algorithm with real-time data would be quite complex. However, I can provide you with a simplified example of how dynamic pricing might work using a hypothetical dataset and algorithm.\n",
        "\n",
        "Here's a basic example using Python:\n"
      ],
      "metadata": {
        "id": "ADGymkGm83zj"
      },
      "id": "ADGymkGm83zj"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Hypothetical demand and supply data\n",
        "demand_data = {\n",
        "    'Hour': range(24),\n",
        "    'Demand': [100, 120, 150, 180, 200, 250, 300, 350, 400, 450, 500, 550,\n",
        "               600, 550, 500, 480, 450, 400, 380, 350, 320, 300, 280, 250]\n",
        "}\n",
        "\n",
        "supply_data = {\n",
        "    'Hour': range(24),\n",
        "    'Supply': [80, 90, 110, 140, 160, 200, 240, 280, 320, 360, 400, 440,\n",
        "               480, 440, 400, 380, 360, 320, 300, 280, 260, 240, 220, 200]\n",
        "}\n",
        "\n",
        "demand_df = pd.DataFrame(demand_data)\n",
        "supply_df = pd.DataFrame(supply_data)\n",
        "\n",
        "# Combine demand and supply data\n",
        "demand_supply_df = pd.merge(demand_df, supply_df, on='Hour')\n",
        "\n",
        "# Function to calculate dynamic pricing\n",
        "def dynamic_pricing(hourly_demand, hourly_supply, base_price):\n",
        "    if hourly_demand > hourly_supply:\n",
        "        # High demand, increase price\n",
        "        return base_price * (1 + 0.1)  # Increase by 10%\n",
        "    elif hourly_demand < hourly_supply:\n",
        "        # Low demand, decrease price\n",
        "        return base_price * (1 - 0.1)  # Decrease by 10%\n",
        "    else:\n",
        "        return base_price  # Maintain base price\n",
        "\n",
        "# Apply dynamic pricing to each hour\n",
        "demand_supply_df['DynamicPrice'] = demand_supply_df.apply(lambda row: dynamic_pricing(row['Demand'], row['Supply'], 10), axis=1)\n",
        "\n",
        "# Display dynamic pricing data\n",
        "print(demand_supply_df)\n"
      ],
      "metadata": {
        "id": "F25efTff88GN"
      },
      "id": "F25efTff88GN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example:\n",
        "\n",
        "- `demand_data` and `supply_data` represent hypothetical demand and supply data for each hour of the day.\n",
        "- The `dynamic_pricing` function calculates the dynamic price based on the relationship between demand and supply. If demand exceeds supply, the price is increased by 10%; if supply exceeds demand, the price is decreased by 10%; otherwise, the base price is maintained.\n",
        "- The `apply` function is used to apply the dynamic pricing function to each row of the DataFrame.\n",
        "\n",
        "In a real-world scenario, Uber would have access to live data streams of demand and supply from their platform, and their algorithm would likely be much more sophisticated. This example provides a simplified illustration of how dynamic pricing might be implemented in Python.\n"
      ],
      "metadata": {
        "id": "TBdIQbJF9Jma"
      },
      "id": "TBdIQbJF9Jma"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conclusion\n",
        "- Business analytics is a crucial component for modern businesses aiming to use data-driven insights to make informed decisions.\n",
        "- Understanding the different types of analytics helps businesses apply the right techniques for descriptive, predictive, and prescriptive insights.\n",
        "- By leveraging advanced analytics, companies can improve operational efficiency, enhance customer satisfaction, and achieve strategic objectives.\n"
      ],
      "metadata": {
        "id": "7n4NNmu19QFU"
      },
      "id": "7n4NNmu19QFU"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}